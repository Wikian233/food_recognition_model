                 from  n    params  module                                  arguments
  0                -1  1       928  models.common.conv_bn_relu_maxpool      [3, 32]
  1                -1  1      8812  models.common.Shuffle_Block             [32, 120, 2]
  2                -1  3     24300  models.common.Shuffle_Block             [120, 120, 1]
  3                -1  1     44588  models.common.Shuffle_Block             [120, 232, 2]
  4                -1  7    200564  models.common.Shuffle_Block             [232, 232, 1]
  5                -1  1    167968  models.common.Shuffle_Block             [232, 464, 2]
  6                -1  3    333384  models.common.Shuffle_Block             [464, 464, 1]
  7                -1  1     59648  models.common.Conv                      [464, 128, 1, 1]
  8                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
  9           [-1, 4]  1         0  models.common.Concat                    [1]
 10                -1  1    104192  models.common.C3                        [360, 128, 1, False]
 11                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
 12                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 13           [-1, 2]  1         0  models.common.Concat                    [1]
 14                -1  1     26496  models.common.C3                        [184, 64, 1, False]
 15                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]
 16          [-1, 11]  1         0  models.common.Concat                    [1]
 17                -1  1     74496  models.common.C3                        [128, 128, 1, False]
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 19           [-1, 7]  1         0  models.common.Concat                    [1]
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 21      [14, 17, 20]  1    104181  models.yolo.Detect                      [72, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]
/home/weijian/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809662/work/aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model Summary: 308 layers, 1639029 parameters, 1639029 gradients, 4.0 GFLOPS
Transferred 474/482 items from pretrained/v5lite-s.pt
Scaled weight_decay = 0.0005
Optimizer groups: 82 .bias, 82 conv.weight, 79 other
[34m[1mtrain: [39m[22mScanning 'train' images and labels... 703 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆ| 703/703 [0
[34m[1mtrain: [39m[22mNew cache created: train.cache
Scanning images:   0%|                                                             | 0/302 [00:00<?, ?it/s]
[34m[1mval: [39m[22mScanning 'test' images and labels... 302 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆ| 302/302 [00:0
[34m[1mval: [39m[22mNew cache created: test.cache
Images sizes do not match. This will causes images to be display incorrectly in the UI.
Image sizes 640 train, 640 test
Using 8 dataloader workers
Logging results to runs/train/exp2
Starting training for 300 epochs...
     Epoch   gpu_mem       box       obj       cls     total    labels  img_size
  0%|                                                                               | 0/44 [00:00<?, ?it/s]
  0%|                                                                               | 0/44 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/home/weijian/Deep_Learning_ws/food_recognition_model/YOLOv5-Lite/train.py", line 544, in <module>
    train(hyp, opt, device, tb_writer)
  File "/home/weijian/Deep_Learning_ws/food_recognition_model/YOLOv5-Lite/train.py", line 305, in train
    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
  File "/home/weijian/Deep_Learning_ws/food_recognition_model/YOLOv5-Lite/utils/loss.py", line 117, in __call__
    tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets
  File "/home/weijian/Deep_Learning_ws/food_recognition_model/YOLOv5-Lite/utils/loss.py", line 211, in build_targets
    indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices
RuntimeError: result type Float can't be cast to the desired output type long int